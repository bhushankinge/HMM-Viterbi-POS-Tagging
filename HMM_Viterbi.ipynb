{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "f479ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "67efcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_text_data(file_path, get_vocabulary=False, separate_sentences=False, replace_unknown=False, vocab=None, tags_present=True):\n",
    "    text_file = open(file_path, 'r')\n",
    "    lines = text_file.readlines()\n",
    "    lines.append(\"\\n\")\n",
    "    data = []\n",
    "    final_data = []\n",
    "    temp = [['0', '<START>', '<START_TAG>']]\n",
    "    if replace_unknown:\n",
    "        known_words = set(vocab.unique_tokens.tolist())\n",
    "    if get_vocabulary:\n",
    "        vocabulary = []\n",
    "    for line in lines:\n",
    "        if line == '\\n':\n",
    "            data.append(temp)\n",
    "            temp = [['0', '<START>', '<START_TAG>']]\n",
    "        else:\n",
    "            line = line.split('\\t')\n",
    "            if get_vocabulary:\n",
    "                vocabulary.append(line[1])\n",
    "            line[-1] = line[-1].strip('\\n')\n",
    "            if replace_unknown and line[1] not in known_words:\n",
    "                line[1] = \"<UNK>\"\n",
    "            temp.append(line)\n",
    "    if get_vocabulary:\n",
    "        vocabulary = pd.DataFrame(vocabulary, columns=[\"token\"])\n",
    "    for data_sample in data:\n",
    "        final_data.append(pd.DataFrame(data_sample, columns=['index', 'token', 'tag']))\n",
    "    if not tags_present:\n",
    "        final_data[-1] = final_data[-1].drop(columns='tag')\n",
    "    combined_data = pd.concat(final_data)\n",
    "    if get_vocabulary and not separate_sentences:\n",
    "        return combined_data, vocabulary\n",
    "    elif get_vocabulary and separate_sentences:\n",
    "        return final_data, vocabulary\n",
    "    elif not get_vocabulary and not separate_sentences:\n",
    "        return combined_data\n",
    "    else:\n",
    "        return final_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "8a55323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##VOCAB Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "5e49ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_vocab = load_text_data('data/train', get_vocabulary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "6eec1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(text_data, vocab_data, threshold=2):\n",
    "    vocab_no_dups = vocab_data['token'].value_counts()\n",
    "    vocab_no_dups = pd.DataFrame(list(zip(vocab_no_dups.index.tolist(), vocab_no_dups.tolist())), columns=['unique_tokens', 'count'])\n",
    "    total_before = len(vocab_no_dups)\n",
    "    to_remove = vocab_no_dups[vocab_no_dups['count'] < threshold]\n",
    "    total_removed = len(to_remove)\n",
    "    vocabulary = vocab_no_dups.drop(vocab_no_dups[vocab_no_dups['count'] < threshold].index)\n",
    "    vocabulary.loc[-1] = ['<UNK>', total_removed]\n",
    "    vocabulary.index = vocabulary.index + 1\n",
    "    vocabulary.sort_index(inplace=True)\n",
    "    vocab_size = len(vocabulary)\n",
    "    token_counts = text_data['token'].value_counts()\n",
    "    text_data['token'] = np.where(text_data['token'].isin(token_counts.index[token_counts < threshold]), '<UNK>', text_data['token'])\n",
    "    print(f\"Threshold: {threshold}\\nVocabulary Size: {vocab_size}\\n<UNK> Occurrences: {total_removed}\")\n",
    "    vocabulary['index'] = vocabulary.index\n",
    "    return text_data, vocabulary, vocab_size, total_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "34a465ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 2\n",
      "Vocabulary Size: 23183\n",
      "<UNK> Occurrences: 20011\n"
     ]
    }
   ],
   "source": [
    "train_data, vocab, vocab_size, total_words_before = build_vocabulary(train_data, train_vocab, threshold=2)\n",
    "vocab.to_csv(\"data/vocab.txt\", index=None, header=None, sep='\\t', columns=['unique_tokens', 'index', 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "70900921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel:\n",
    "    def __init__(self, lexicon=None, data=None) -> None:\n",
    "        self.lexicon = lexicon\n",
    "        self.data = data\n",
    "\n",
    "    def generate_helpers(self):\n",
    "        self.data_records = self.data.to_dict('records')\n",
    "        #print(self.data_records[:100])\n",
    "        self.unique_tokens = self.lexicon['unique_tokens'].to_numpy()\n",
    "        self.data['tag'].value_counts()\n",
    "        self.label_freqs = dict(zip(self.data['tag'].value_counts().index.tolist(), self.data['tag'].value_counts().tolist()))\n",
    "\n",
    "        self.labels = self.data['tag'].unique().tolist()\n",
    "        self.tokens = self.lexicon['unique_tokens'].tolist()\n",
    "        self.tokens.append(\"<START>\")\n",
    "\n",
    "    def train(self):\n",
    "        self.generate_helpers()\n",
    "\n",
    "        self.transition_freqs = {key:{key2:0 for key2 in self.labels} for key in self.labels}\n",
    "        self.emission_freqs = {key:{key2:0 for key2 in self.tokens} for key in self.labels}\n",
    "\n",
    "        prev_label = '<START_TAG>'\n",
    "        temp_record = {'index': '0', 'token': '<START>', 'tag': '<START_TAG>'}\n",
    "\n",
    "        for record in self.data_records:\n",
    "            self.emission_freqs[str(record['tag'])][str(record['token'])] += 1\n",
    "            self.transition_freqs[str(prev_label)][str(record['tag'])] += 1\n",
    "\n",
    "            prev_label = record['tag']\n",
    "            temp_record = record\n",
    "\n",
    "        self.emission_freqs['<START_TAG>']['<UNK>'] = self.label_freqs['<START_TAG>']\n",
    "        self.transition_freqs['<START_TAG>']['<START_TAG>'] -= 1\n",
    "\n",
    "        self.compute_probabilities()\n",
    "        self.convert_probabilities_for_output()\n",
    "\n",
    "        print(f\"No. of Transition Parameters: {len(self.transition_probs_output)} \\nNo. of Emission Parameters: {len(self.emission_probs_output)}\")\n",
    "\n",
    "    def compute_probabilities(self):\n",
    "        self.transition_totals = {key: sum(self.transition_freqs[str(key)].values()) for key in self.labels}\n",
    "        self.emission_totals = {key: sum(self.emission_freqs[str(key)].values()) for key in self.labels}\n",
    "\n",
    "        transition_temp = {}\n",
    "        emission_temp = {}\n",
    "\n",
    "        for key in self.labels:\n",
    "            transition_temp[str(key)] = (self.label_freqs[str(key)] - self.transition_totals[str(key)])\n",
    "            emission_temp[str(key)] = (self.label_freqs[str(key)] - self.emission_totals[str(key)])\n",
    "\n",
    "        self.label_freqs_new = {key: val - emission_temp[str(key)] for key, val in self.label_freqs.items()}\n",
    "\n",
    "        self.transition_probs = {key: {key2: val / self.label_freqs[str(key)] for key2, val in self.transition_freqs[str(key)].items()} for key in self.labels}\n",
    "        self.emission_probs = {key: {key2: val / self.label_freqs_new[str(key)] for key2, val in self.emission_freqs[str(key)].items()} for key in self.labels}\n",
    "\n",
    "    def convert_probabilities_for_output(self):\n",
    "        self.transition_probs_output = {}\n",
    "        self.emission_probs_output = {}\n",
    "\n",
    "        for label1 in self.transition_probs.keys():\n",
    "            for label2 in self.transition_probs[label1].keys():\n",
    "                prob = self.transition_probs[label1][label2]\n",
    "                if prob != 0:\n",
    "                    self.transition_probs_output[f\"({label1}, {label2})\"] = prob\n",
    "\n",
    "            for token in self.emission_probs[label1].keys():\n",
    "                prob = self.emission_probs[label1][token]\n",
    "                if prob != 0:\n",
    "                    self.emission_probs_output[f\"({label1}, {token})\"] = prob\n",
    "\n",
    "    def convert_output_to_probabilities(self):\n",
    "        self.transition_probs = {key:{key2:0 for key2 in self.labels} for key in self.labels}\n",
    "        self.emission_probs = {key:{key2:0 for key2 in self.tokens} for key in self.labels}\n",
    "\n",
    "        for pair, prob in self.transition_probs_output.items():\n",
    "            label1, label2 = pair[1:-1].split(\" \")\n",
    "            self.transition_probs[label1[:-1]][label2] = prob\n",
    "\n",
    "        for pair, prob in self.emission_probs_output.items():\n",
    "            label, token = pair[1:-1].split(\" \")\n",
    "            self.emission_probs[label[0:-1]][token] = prob\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath) as json_file:\n",
    "            model = json.load(json_file)\n",
    "\n",
    "        self.transition_probs_output = model['transition']\n",
    "        self.emission_probs_output = model['emission']\n",
    "        self.convert_output_to_probabilities()\n",
    "        self.labels = list(self.transition_probs.keys())\n",
    "\n",
    "        return self.transition_probs, self.emission_probs\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        model = {'transition': self.transition_probs_output, 'emission': self.emission_probs_output}\n",
    "\n",
    "        with open(filepath, \"w\") as json_file:\n",
    "            json.dump(model, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "6e991f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Transition Parameters: 1416 \n",
      "No. of Emission Parameters: 30305\n"
     ]
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel(vocab, train_data)\n",
    "hmm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "4ede16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.save_model(\"data/hmm.json\")\n",
    "transition_prob, emission_prob = hmm.load_model(\"data/hmm.json\")\n",
    "#trans_prob  = hmm.transition_probs_output\n",
    "#emis_prob   = hmm.emission_probs_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "d68132a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Greedy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "3fd048c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyDecoder:\n",
    "    def __init__(self, data, labels, transition_probs, emission_probs, original_data):\n",
    "        \"\"\"\n",
    "        Initializes a GreedyDecoder instance.\n",
    "        \n",
    "        Parameters:\n",
    "            data (list): List of sentences to be decoded.\n",
    "            labels (list): List of tags for each sentence.\n",
    "            transition_probs (dict): Transition probabilities between labels.\n",
    "            emission_probs (dict): Emission probabilities for each label.\n",
    "            original_data (list): Original data used to create the model.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.original_data = original_data\n",
    "        self.transition_probs = transition_probs\n",
    "        self.emission_probs = emission_probs\n",
    "        self.labels = labels\n",
    "\n",
    "    def decode_sentence(self, sentence_df):\n",
    "        \"\"\"\n",
    "        Decodes a single sentence using the greedy algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "            sentence_df (pandas.DataFrame): Sentence to be decoded.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Tuple containing the predicted labels and the original sentence.\n",
    "        \"\"\"\n",
    "        previous_label = '<START_TAG>'\n",
    "        #print(sentence_df)\n",
    "        sentence = sentence_df['token'].values.tolist()[1:]\n",
    "        self.sent_predictions = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        probability=1\n",
    "        for token in sentence:\n",
    "            max_probability = -1\n",
    "            for label in self.labels:\n",
    "                #print(self.transition_probs[previous_label][label])\n",
    "                probability = self.transition_probs[previous_label][label] * self.emission_probs[label][token]\n",
    "                \n",
    "                if probability > max_probability:\n",
    "                    max_probability = probability\n",
    "                    predicted_label = label\n",
    "            \n",
    "            previous_label = predicted_label\n",
    "            self.sent_predictions.append(predicted_label)\n",
    "        \n",
    "        return self.sent_predictions, sentence\n",
    "    \n",
    "    def decode(self):\n",
    "        \"\"\"\n",
    "        Decodes all sentences in the data using the greedy algorithm.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Tuple containing the predicted labels, original tokens, and the output text.\n",
    "        \"\"\"\n",
    "        self.predictions = []\n",
    "        original_tokens = []\n",
    "        self.output_text = []\n",
    "        \n",
    "        counter=0\n",
    "        for sentence in self.data:\n",
    "            original_sentence = self.original_data[counter]['token'].values.tolist()[1:]\n",
    "            #print(original_sentence)\n",
    "            #print(sentence)\n",
    "            predictions, sentence = self.decode_sentence(sentence)\n",
    "            \n",
    "            position = 1\n",
    "            \n",
    "            for predicted_label, token in zip(predictions, original_sentence):\n",
    "                self.output_text.append(f\"{position}\\t{token}\\t{predicted_label}\\n\")\n",
    "                position += 1\n",
    "                \n",
    "            original_tokens.extend(original_sentence)\n",
    "            self.predictions.extend(predictions)\n",
    "            self.output_text.append(\"\\n\")\n",
    "        \n",
    "        self.output_text = \"\".join(self.output_text[:-1])\n",
    "        return self.predictions\n",
    "    \n",
    "    def calculate_accuracy(self, targets):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of the model on the given targets.\n",
    "        \n",
    "        Parameters:\n",
    "            targets (list): List of true labels for each sentence.\n",
    "        \n",
    "        Returns:\n",
    "            float: Accuracy of the model on the given targets.\n",
    "        \"\"\"\n",
    "        match_count = 0\n",
    "        \n",
    "        for prediction, target in zip(self.predictions, targets):\n",
    "            if prediction == target:\n",
    "                match_count += 1\n",
    "        \n",
    "        accuracy = match_count / len(self.predictions)\n",
    "        return accuracy\n",
    "    \n",
    "    def get_targets(self):\n",
    "        \"\"\"\n",
    "        Gets the true labels for each sentence in the data.\n",
    "        \n",
    "        Returns:\n",
    "            list: List of true labels for each sentence.\n",
    "        \"\"\"\n",
    "        targets = []\n",
    "        \n",
    "        for sentence in self.data:\n",
    "            targets.extend(sentence['tag'].values.tolist()[1:])\n",
    "        \n",
    "        return targets\n",
    "    \n",
    "    def save_predictions(self, filepath):\n",
    "        \"\"\"\n",
    "        Saves the predicted labels and original tokens to a file.\n",
    "        \n",
    "        Parameters:\n",
    "            filepath (str): Path to the file where the predictions will be saved.\n",
    "        \"\"\"\n",
    "        with open(filepath, \"w\") as output_file:\n",
    "            output_file.write(self.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "7bac1942",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = load_text_data('data/dev', get_vocabulary=False,separate_sentences=True, replace_unknown=True, vocab=hmm.lexicon)\n",
    "dev_data_orig = load_text_data('data/dev', get_vocabulary=False, separate_sentences=True,  vocab=hmm.lexicon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "4f3d1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_dev = GreedyDecoder(dev_data, hmm.labels, transition_prob, emission_prob, dev_data_orig)\n",
    "preds = greedy_dev.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "453e5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_prob, emission_prob = hmm.load_model(\"data/hmm.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "b4e9b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Decoding Accuracy on dev_data: 93.4870378240544\n"
     ]
    }
   ],
   "source": [
    "acc = greedy_dev.calculate_accuracy(greedy_dev.get_targets())\n",
    "print(f\"Greedy Decoding Accuracy on dev_data: {acc*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cae5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##greedy test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "f9e6be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_text_data('data/test', get_vocabulary=False,separate_sentences=True, replace_unknown=True, vocab=hmm.lexicon)\n",
    "test_data_orig = load_text_data('data/test', get_vocabulary=False, separate_sentences=True,  vocab=hmm.lexicon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "1ad8621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_prob, emission_prob = hmm.load_model(\"data/hmm.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "f3a9b660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "greedy_test= GreedyDecoder(test_data, hmm.labels, transition_prob, emission_prob, test_data_orig)\n",
    "preds = greedy_test.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "a0d3f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_test.save_predictions(\"data/greedy.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "6184a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "713a6ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ViterbiDecoding:\n",
    "    def __init__(self, data, tag_list, transition_prob, emission_prob, data_orig) -> None:\n",
    "        self.input_data = data\n",
    "        self.original_data = data_orig\n",
    "        self.transition_probs = transition_prob\n",
    "        self.emission_probs = emission_prob\n",
    "        self.tag_list = tag_list[1:]\n",
    "        self.map_tag_to_index()\n",
    "        self.map_index_to_tag()\n",
    "        pass\n",
    "\n",
    "    def map_tag_to_index(self):\n",
    "        self.tag_to_index = {}\n",
    "        i = 0\n",
    "        for tag in self.tag_list:\n",
    "            self.tag_to_index[tag] = i\n",
    "            i += 1\n",
    "\n",
    "    def map_index_to_tag(self):\n",
    "        self.index_to_tag = {v: k for k, v in self.tag_to_index.items()}\n",
    "\n",
    "    def predict(self):\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        self.output_string = \"\"\n",
    "\n",
    "        count = 0\n",
    "        for sentence in self.input_data:\n",
    "            sentence_orig = self.original_data[count]['token'].values.tolist()[1:]\n",
    "            \n",
    "            predictions, sentence = self.predict_sentence(sentence)\n",
    "            pos = 1\n",
    "            for pred, word in zip(predictions, sentence_orig):\n",
    "                self.output_string += f\"{pos}\\t{word}\\t{pred}\\n\"\n",
    "                pos += 1\n",
    "            self.predictions.extend(predictions)\n",
    "            \n",
    "            self.output_string += \"\\n\"\n",
    "            \n",
    "            count += 1\n",
    "            # if count % 500 == 0:\n",
    "            #     print(f\"Completed {count} sentences.\")\n",
    "        \n",
    "        self.output_string = \"\".join(self.output_string[:-1])\n",
    "        \n",
    "        return self.predictions\n",
    "\n",
    "    def calculate_accuracy(self, targets):\n",
    "        count_of_matches = 0\n",
    "        for pred, target in zip(self.predictions, targets):\n",
    "            if pred == target:\n",
    "                count_of_matches += 1\n",
    "\n",
    "        self.accuracy = count_of_matches / len(self.predictions)\n",
    "        return self.accuracy\n",
    "    \n",
    "    def get_targets(self):\n",
    "        self.targets = []\n",
    "        for sentence_df in self.input_data:\n",
    "            self.targets.extend(sentence_df['tag'].values.tolist()[1:])\n",
    "        return self.targets\n",
    "\n",
    "    \n",
    "    def predict_sentence(self, sentence_df):\n",
    "        self.sentence = sentence_df['token'].values.tolist()[1:]\n",
    "\n",
    "        sentence_length = len(self.sentence)\n",
    "        no_of_tags = len(self.tag_list)\n",
    "\n",
    "        self.option_probabilities = np.zeros((no_of_tags, sentence_length))\n",
    "        self.backtrack_matrix = np.zeros((no_of_tags, sentence_length))\n",
    "\n",
    "        index = 0\n",
    "        for tag in self.tag_list:\n",
    "            self.option_probabilities[self.tag_to_index[tag], index] = self.transition_probs['<START_TAG>'][tag] * self.emission_probs[tag][self.sentence[index]]\n",
    "\n",
    "        for j in range(1, sentence_length):\n",
    "            for current_tag in self.tag_list:\n",
    "                temp_prob = []\n",
    "                for previous_tag in self.tag_list:\n",
    "                    temp_prob.append(self.option_probabilities[self.tag_to_index[previous_tag], j-1] * self.transition_probs[previous_tag][current_tag] * self.emission_probs[current_tag][self.sentence[j]])\n",
    "                \n",
    "                max_tag_index = np.argmax(temp_prob)\n",
    "\n",
    "                self.option_probabilities[self.tag_to_index[current_tag], j] = temp_prob[max_tag_index]\n",
    "                self.backtrack_matrix[self.tag_to_index[current_tag], j ] = max_tag_index\n",
    "\n",
    "        predicted_tags = self.backtrack(self.option_probabilities, self.backtrack_matrix)\n",
    "\n",
    "        return (predicted_tags, self.sentence)\n",
    "\n",
    "    def backtrack(self, option_probabilities, backtrack_matrix):\n",
    "        predicted_tag = []\n",
    "        sentence_length = len(self.sentence)\n",
    "        no_of_tags = len(self.tag_list)\n",
    "        \n",
    "        j = sentence_length - 1\n",
    "        index = np.argmax(option_probabilities[:,j])\n",
    "        pointer = backtrack_matrix[index, j]\n",
    "        predicted_tag.append(self.index_to_tag[index])\n",
    "\n",
    "        for j in range(sentence_length-2, -1, -1):\n",
    "            predicted_tag.append(self.index_to_tag[pointer])\n",
    "            pointer = backtrack_matrix[int(pointer), j]\n",
    "\n",
    "        predicted_tag.reverse()\n",
    "\n",
    "        return predicted_tag\n",
    "    \n",
    "    def write_prediction_into_file(self, filepath):\n",
    "        with open(filepath, \"w\") as output_file:\n",
    "            output_file.write(self.output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "3ff77931",
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_dev = ViterbiDecoding(dev_data, hmm.labels, transition_prob, emission_prob, dev_data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "784cd6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = viterbi_dev.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "5ca1e631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Decoding Accuracy on dev_data: 94.76883613623946\n"
     ]
    }
   ],
   "source": [
    "acc = viterbi_dev.calculate_accuracy(viterbi_dev.get_targets())\n",
    "print(f\"Viterbi Decoding Accuracy on dev_data: {acc*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##viterbi test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "0ec85d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_test = ViterbiDecoding(test_data, hmm.labels, transition_prob, emission_prob, test_data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "7e03e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = viterbi_test.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "02bedead",
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_test.write_prediction_into_file(\"data/viterbi.out\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
